import os
import gc
import psutil
import random
import datetime as dt
from pathlib import Path

import requests
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel
from telebot import TeleBot, apihelper
from pynvml import (nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetUtilizationRates,
                    nvmlDeviceGetTemperature, nvmlDeviceGetMemoryInfo, nvmlShutdown)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_MODEL_ID     = "mistralai/Mistral-7B-Instruct-v0.3"

LORA_ADAPTER_DIR  = "models/vlad/final_adapter"

SYSTEM_PROMPT    = "Ğ¢ĞµĞ±Ñ Ğ·Ğ¾Ğ²ÑƒÑ‚ Ğ’Ğ»Ğ°Ğ´. Ğ¢Ñ‹ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº, ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ² ÑÑ„ĞµÑ€Ğµ IT. ĞŸĞ¸ÑˆĞ¸ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹, Ğ¼Ğ¾Ñ‚Ğ¸Ğ²Ğ¸Ñ€ÑƒĞ¹ Ğ¿Ñ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°Ñ‚ÑŒ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³."
USER_INSTRUCTION_TEMPLATE = "Ğ˜Ğ¼Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: {who}. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ: {text}"
MAX_CONTEXT_TOKENS = 2048
MAX_HISTORY_MESSAGES = 40
ADMIN_ID          = 304622290
ADMIN_CHAT_ID     = 304622290  
TELEGRAM_TOKEN    = "667589363:AAFIFSIh3Yyy2dyratXGwaCP2bAkc8DI-tY"

DEVICE            = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE             = torch.float16 if DEVICE == "cuda" else torch.float32

MAX_NEW_TOKENS    = 128  
TEMPERATURE            = 0.7
TOP_P                  = 0.8        
WHOO = "ĞĞ°Ñ‚Ğ°Ğ»ÑŒÑ Ğ¡Ğ¾Ğ¸Ğ½Ğ°"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ram_mb() -> float:
    return psutil.Process(os.getpid()).memory_info().rss / 2**20

def gpu_info() -> str:
    if DEVICE != "cuda":
        return "GPU Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚"
    try:
        nvmlInit()
        h       = nvmlDeviceGetHandleByIndex(0)
        util    = nvmlDeviceGetUtilizationRates(h)
        temp    = nvmlDeviceGetTemperature(h, 0)
        mem     = nvmlDeviceGetMemoryInfo(h)
        nvmlShutdown()
        mb = lambda x: x / 2**20
        return (f"GPU util: {util.gpu}%\n"
                f"tÂ°: {temp}Â°C\n"
                f"mem used: {mb(mem.used):.0f}/{mb(mem.total):.0f} MB")
    except Exception as e:
        return f"NVML-Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}"

def safe_send(bot: TeleBot, chat_id: int, text: str, *args, **kw):
    """Ğ”ĞµĞ»Ğ¸Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ â‰¤ 4096 ÑĞ¸Ğ¼Ğ². (Ğ±ĞµĞ· Markdown)."""
    for chunk in (text[i:i+4000] for i in range(0, len(text), 4000)):
        bot.send_message(chat_id, chunk, *args, **kw)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒâ€¦")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=False)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype=DTYPE,
    device_map={"": DEVICE},
)
base_model.resize_token_embeddings(len(tokenizer))          # safety

print("ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°Ñ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€â€¦")
model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_DIR)
#model = base_model
model.eval()

GEN_CFG = GenerationConfig(
    max_new_tokens = MAX_NEW_TOKENS,
    temperature    = TEMPERATURE,
    top_p          = TOP_P,
    do_sample      = True,
    eos_token_id   = tokenizer.eos_token_id,
    pad_token_id   = tokenizer.pad_token_id,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Telegram-Ğ±Ğ¾Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bot = TeleBot(TELEGRAM_TOKEN, parse_mode=None)

DIALOGS = {}                 # user_id â†’ list[dict(role, content)]

def reset_dialog(uid: int):
    DIALOGS[uid] = [{"role": "system", "content": SYSTEM_PROMPT}]

def trim_history(uid: int) -> list[dict]:
    history = DIALOGS.get(uid, [])
    if not history:
        return []
    head = history[:1] if history[0].get("role") == "system" else []
    tail = history[1:]
    if len(tail) > MAX_HISTORY_MESSAGES:
        tail = tail[-MAX_HISTORY_MESSAGES:]
    history = head + tail
    DIALOGS[uid] = history
    return history

def build_prompt(current_user_text: str, who: str) -> str:
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ñ‚Ğ°Ğº Ğ¶Ğµ, ĞºĞ°Ğº Ğ² Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ¾Ğ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ."""
    user_text = USER_INSTRUCTION_TEMPLATE.format(who=who, text=current_user_text)
    parts = [
        f"[system]{SYSTEM_PROMPT}[/system]",
        f"[user]{user_text}[/user]",
    ]
    return "\n".join(parts) + "\n[assistant]"

@torch.inference_mode()
def llm_answer(user_id: int, text: str, who: str) -> str:
    if user_id not in DIALOGS:
        reset_dialog(user_id)
    DIALOGS[user_id].append({"role": "user", "content": text})
    trim_history(user_id)
    prompt = build_prompt(text, who)
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=MAX_CONTEXT_TOKENS,
    ).to(model.device)
    output_ids = model.generate(**inputs, generation_config=GEN_CFG)[0]
    answer_ids = output_ids[inputs.input_ids.shape[1]:]
    answer = tokenizer.decode(answer_ids, skip_special_tokens=True).strip()
    DIALOGS[user_id].append({"role": "assistant", "content": answer})
    trim_history(user_id)
    return answer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@bot.message_handler(commands=["start", "help"])
def cmd_help(msg):
    safe_send(bot, msg.chat.id,
              "/help â€“ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹\n/clear â€“ Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚\n/info â€“ Ñ€ĞµÑÑƒÑ€ÑÑ‹\n/kill â€“ Ğ²Ñ‹ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ (Ğ°Ğ´Ğ¼.)")

@bot.message_handler(commands=["clear"])
def cmd_clear(msg):
    reset_dialog(msg.from_user.id)
    bot.reply_to(msg, "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½.")

@bot.message_handler(commands=["info"])
def cmd_info(msg):
    mem = ram_mb()
    gpu = gpu_info()
    safe_send(bot, msg.chat.id, f"RAM: {mem:.0f} MB\n{gpu}")


@bot.message_handler(commands=["who"])
def cmd_who(message):
    global WHOO
    response = f"Ğ¢Ğ°ĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½ĞµÑ‚!\nĞ”Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:\n/help - Ğ”Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹\n/login pass - ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ\n/clear - ĞÑ‚Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°"
    cmd = message.text.split()[0]
    args = message.text.split()[1:] 
    if cmd == '/who':
        response = f"Ğ’Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ /who Ñ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: {' '.join(args)}"
        if len(args)>0:
                WHOO = ' '.join(args)
                response = f"Ğ’Ñ‹ {WHOO}"
        else:
                response = f"Ğ’Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ /who Ñ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: {' '.join(args)}"
    safe_send(bot, message.chat.id, response)

@bot.message_handler(commands=["kill"])
def cmd_kill(msg):
    if msg.from_user.id != ADMIN_ID:
        bot.reply_to(msg, "ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ².")
        return
    bot.reply_to(msg, "Ğ’Ñ‹ĞºĞ»ÑÑ‡Ğ°ÑÑÑŒâ€¦")
    bot.stop_polling()
    gc.collect()
    if DEVICE == "cuda":
        torch.cuda.empty_cache()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ…ÑĞ½Ğ´Ğ»ĞµÑ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@bot.message_handler(content_types=["text"])
def handle_text(msg):
   # print(msg)
    uid = msg.from_user.id
    who = WHOO or msg.from_user.first_name or "user"
    print(f"[{uid}:{who}]\n{msg.text}")
    #if uid != ADMIN_ID:
       # bot.reply_to(msg, "Ğ‘Ğ¾Ñ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ñƒ.")
       # return

    try:
        start = dt.datetime.now()
        answer = llm_answer(uid, msg.text,who)
        safe_send(bot, uid, answer)
        dur = dt.datetime.now() - start
        print(f"[{uid}] {dur.total_seconds():.1f}s â‡’ {len(answer)} ÑĞ¸Ğ¼Ğ².")
    except Exception as e:
        bot.reply_to(msg, f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    bot.send_message(ADMIN_ID, "ğŸ¤– Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ.")
    bot.infinity_polling(skip_pending=True)
