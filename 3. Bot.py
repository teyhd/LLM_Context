import os
import gc
import psutil
import random
import logging
import datetime as dt
from pathlib import Path

import requests
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
from peft import PeftModel
from telebot import TeleBot, apihelper
from pynvml import (nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetUtilizationRates,
                    nvmlDeviceGetTemperature, nvmlDeviceGetMemoryInfo, nvmlShutdown)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_MODEL_ID     = "mistralai/Mistral-7B-Instruct-v0.3"
LORA_ADAPTER_DIR  = "models/vlad3/checkpoint-1000"

SYSTEM_PROMPT    = "Ğ¢Ñ‹ Ğ’Ğ»Ğ°Ğ´. Ğ¢Ñ‹ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ğ¸ Ğ»Ğ°ĞºĞ¾Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹.\nĞ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ Ñ„Ğ¾ĞºÑƒÑ â€” Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞºĞ°: Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¿Ğ¾ Ğ´ĞµĞ»Ñƒ, Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½ĞµĞ¹ Ğ²Ğ¾Ğ´Ñ‹."
USER_INSTRUCTION_TEMPLATE = "Ğ˜Ğ¼Ñ ÑĞ¾Ğ±ĞµÑĞµĞ´Ğ½Ğ¸ĞºĞ°: {who}. ĞĞ°Ğ¿Ğ¸ÑˆĞ¸ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ: {text}"
MAX_CONTEXT_TOKENS = 2048
MAX_HISTORY_MESSAGES = 40
ADMIN_ID          = 304622290
ADMIN_CHAT_ID     = 304622290  
TELEGRAM_TOKEN    = "667589363:AAFIFSIh3Yyy2dyratXGwaCP2bAkc8DI-tY"

DEVICE            = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE             = torch.float16 if DEVICE == "cuda" else torch.float32

MAX_NEW_TOKENS    = 128  
TEMPERATURE       = 0.4
TOP_P             = 0.7        
WHOO = "ĞĞ»Ğ¸ÑĞ° Ğ®Ñ€ÑŒĞµĞ²Ğ½Ğ°"

LOG_FILE = "bot.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_FILE, "a", "utf-8"),
        logging.StreamHandler(),
    ],
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ram_mb() -> float:
    return psutil.Process(os.getpid()).memory_info().rss / 2**20

def gpu_info() -> str:
    if DEVICE != "cuda":
        return "GPU Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚"
    try:
        nvmlInit()
        h       = nvmlDeviceGetHandleByIndex(0)
        util    = nvmlDeviceGetUtilizationRates(h)
        temp    = nvmlDeviceGetTemperature(h, 0)
        mem     = nvmlDeviceGetMemoryInfo(h)
        nvmlShutdown()
        mb = lambda x: x / 2**20
        return (f"GPU util: {util.gpu}%\n"
                f"tÂ°: {temp}Â°C\n"
                f"mem used: {mb(mem.used):.0f}/{mb(mem.total):.0f} MB")
    except Exception as e:
        return f"NVML-Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {e}"

def safe_send(bot: TeleBot, chat_id: int, text: str, *args, **kw):
    """Ğ”ĞµĞ»Ğ¸Ñ‚ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ â‰¤ 4096 ÑĞ¸Ğ¼Ğ². (Ğ±ĞµĞ· Markdown)."""
    for chunk in (text[i:i+4000] for i in range(0, len(text), 4000)):
        bot.send_message(chat_id, chunk, *args, **kw)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ±Ğ°Ğ·Ğ¾Ğ²ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒâ€¦")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=False)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    torch_dtype=DTYPE,
    device_map={"": DEVICE},
)
base_model.resize_token_embeddings(len(tokenizer))          # safety

print("ĞŸĞ¾Ğ´ĞºĞ»ÑÑ‡Ğ°Ñ LoRA-Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€â€¦")
model = PeftModel.from_pretrained(
    base_model,
    LORA_ADAPTER_DIR,
    torch_dtype=DTYPE,
    device_map={"": DEVICE},
)
#model = base_model
model.eval()

GEN_CFG = GenerationConfig(
    max_new_tokens = MAX_NEW_TOKENS,
    temperature    = TEMPERATURE,
    top_p          = TOP_P,
    do_sample      = True,
    eos_token_id   = tokenizer.eos_token_id,
    pad_token_id   = tokenizer.pad_token_id,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Telegram-Ğ±Ğ¾Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
bot = TeleBot(TELEGRAM_TOKEN, parse_mode=None)

DIALOGS = {}                 # user_id â†’ list[dict(role, content)]

def reset_dialog(uid: int):
    DIALOGS[uid] = [{"role": "system", "content": SYSTEM_PROMPT}]

def trim_history(uid: int) -> list[dict]:
    history = DIALOGS.get(uid, [])
    if not history:
        return []
    head = history[:1] if history[0].get("role") == "system" else []
    tail = history[1:]
    if len(tail) > MAX_HISTORY_MESSAGES:
        tail = tail[-MAX_HISTORY_MESSAGES:]
    history = head + tail
    DIALOGS[uid] = history
    return history

def build_chat_messages(messages: list[dict], who: str) -> list[dict]:
    """Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ chat_template Mistral Ğ¸ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº Ñ‡ĞµÑ€ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ user/assistant."""
    templated = []
    for m in messages:
        role = m.get("role")
        content = (m.get("content") or "").strip()
        if not content:
            continue
        if role == "user":
            content = USER_INSTRUCTION_TEMPLATE.format(who=who, text=content)
        templated.append({"role": role, "content": content})

    merged = []
    for m in templated:
        if merged and merged[-1]["role"] == m["role"]:
            merged[-1]["content"] += "\n" + m["content"]
        else:
            merged.append(m)

    has_system = merged and merged[0]["role"] == "system"
    core = merged[1:] if has_system else merged

    while core and core[0]["role"] == "assistant":
        core = core[1:]

    alternated = [{"role": "system", "content": merged[0]["content"]}] if has_system else []
    for m in core:
        if not alternated:
            if m["role"] == "assistant":
                continue
            alternated.append(m)
            continue
        if alternated[-1]["role"] == m["role"]:
            alternated[-1]["content"] += "\n" + m["content"]
        else:
            alternated.append(m)

    while alternated and alternated[-1]["role"] != "user":
        alternated.pop()
    return alternated

def log_context(uid: int, who: str, history: list[dict], prompt: str, prompt_tokens: int) -> None:
    """Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ²ĞµÑÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸ĞµĞ¹, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ, Ñ‡Ñ‚Ğ¾ ÑƒÑˆĞ»Ğ¾ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ."""
    try:
        logging.info(
            "CTX uid=%s who=%s msgs=%d prompt_tokens=%d\n%s",
            uid, who, len(history), prompt_tokens, prompt,
        )
    except Exception as e:  # noqa: BLE001
        logging.warning("log_context failed: %s", e)

@torch.inference_mode()
def llm_answer(user_id: int, text: str, who: str) -> str:
    if user_id not in DIALOGS:
        reset_dialog(user_id)
    DIALOGS[user_id].append({"role": "user", "content": text})
    history = trim_history(user_id)
    chat_ctx = build_chat_messages(history, who)
    if not chat_ctx:
        return "Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¿ÑƒÑÑ‚Ğ°, Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ÑŒÑ‚Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ·."

    # Ğ¡Ñ‚Ñ€Ğ¾Ğ¸Ğ¼ prompt ĞºĞ°Ğº Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ¸ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞ¿Ñ€Ğ°Ğ²Ğ°, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ñ‚ĞµÑ€ÑÑ‚ÑŒ ÑĞ²ĞµĞ¶Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
    prompt_text = tokenizer.apply_chat_template(
        chat_ctx,
        tokenize=False,
        add_generation_prompt=True,
    )
    prompt_ids = tokenizer(
        prompt_text,
        add_special_tokens=False,
    ).input_ids
    if len(prompt_ids) > MAX_CONTEXT_TOKENS:
        prompt_ids = prompt_ids[-MAX_CONTEXT_TOKENS:]
        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)

    inputs = {
        "input_ids": torch.tensor([prompt_ids], device=model.device),
        "attention_mask": torch.ones(1, len(prompt_ids), device=model.device),
    }
    log_context(user_id, who, history, prompt_text, len(prompt_ids))

    output_ids = model.generate(**inputs, generation_config=GEN_CFG)[0]
    answer_ids = output_ids[len(prompt_ids):]
    answer = tokenizer.decode(answer_ids, skip_special_tokens=True).strip()
    DIALOGS[user_id].append({"role": "assistant", "content": answer})
    trim_history(user_id)
    logging.info("ANSWER uid=%s len=%d text=%s", user_id, len(answer), answer)
    return answer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‡Ğ¸ĞºĞ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@bot.message_handler(commands=["start", "help"])
def cmd_help(msg):
    safe_send(bot, msg.chat.id,
              "/help â€“ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹\n/clear â€“ Ğ¾Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚\n/info â€“ Ñ€ĞµÑÑƒÑ€ÑÑ‹\n/kill â€“ Ğ²Ñ‹ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ (Ğ°Ğ´Ğ¼.)")

@bot.message_handler(commands=["clear"])
def cmd_clear(msg):
    reset_dialog(msg.from_user.id)
    bot.reply_to(msg, "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½.")

@bot.message_handler(commands=["info"])
def cmd_info(msg):
    mem = ram_mb()
    gpu = gpu_info()
    safe_send(bot, msg.chat.id, f"RAM: {mem:.0f} MB\n{gpu}")


@bot.message_handler(commands=["who"])
def cmd_who(message):
    global WHOO
    response = f"Ğ¢Ğ°ĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹ Ğ½ĞµÑ‚!\nĞ”Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:\n/help - Ğ”Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ¼Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹\n/login pass - ĞĞ²Ñ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ\n/clear - ĞÑ‚Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°"
    cmd = message.text.split()[0]
    args = message.text.split()[1:] 
    if cmd == '/who':
        response = f"Ğ’Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ /who Ñ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: {' '.join(args)}"
        if len(args)>0:
                WHOO = ' '.join(args)
                response = f"Ğ’Ñ‹ {WHOO}"
        else:
                response = f"Ğ’Ñ‹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ñƒ /who Ñ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ğ¼Ğ¸: {' '.join(args)}"
    safe_send(bot, message.chat.id, response)

@bot.message_handler(commands=["kill"])
def cmd_kill(msg):
    if msg.from_user.id != ADMIN_ID:
        bot.reply_to(msg, "ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ¿Ñ€Ğ°Ğ².")
        return
    bot.reply_to(msg, "Ğ’Ñ‹ĞºĞ»ÑÑ‡Ğ°ÑÑÑŒâ€¦")
    bot.stop_polling()
    gc.collect()
    if DEVICE == "cuda":
        torch.cuda.empty_cache()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ñ…ÑĞ½Ğ´Ğ»ĞµÑ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@bot.message_handler(content_types=["text"])
def handle_text(msg):
   # print(msg)
    uid = msg.from_user.id
    who = WHOO or msg.from_user.first_name or "user"
    logging.info("IN uid=%s who=%s text=%s", uid, who, msg.text)
    #if uid != ADMIN_ID:
       # bot.reply_to(msg, "Ğ‘Ğ¾Ñ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ñƒ.")
       # return

    try:
        start = dt.datetime.now()
        answer = llm_answer(uid, msg.text, who)
        safe_send(bot, uid, answer)
        dur = dt.datetime.now() - start
        logging.info("OUT uid=%s dur=%.1fs len=%d", uid, dur.total_seconds(), len(answer))
    except Exception as e:
        bot.reply_to(msg, f"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
        raise

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    bot.send_message(ADMIN_ID, "ğŸ¤– Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ğ»ÑÑ Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ² Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğµ.")
    bot.infinity_polling(skip_pending=True)
