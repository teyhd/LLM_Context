Test
LORA_R                 = 32#8
LORA_ALPHA             = 64#16
LORA_DROPOUT           = 0.2

# Блоки, к которым будет применяться LoRA (типичный набор для Mistral)
TARGET_MODULES = [
    "q_proj",
   # "k_proj",
    "v_proj",
   # "o_proj",
    "gate_proj",
   # "up_proj",
   # "down_proj",
]

Train/Inference Check Report
============================

Контекст
- Модель: mistralai/Mistral-7B-Instruct-v0.3 + LoRA.
- Лимит токенов: MAX_SEQ_LEN=2048 (обучение), MAX_CONTEXT_TOKENS=2048 (бот).
- Датасет: data/output/train.jsonl (3891 диалог) и valid.jsonl (671 диалог).

Формирование данных (2. Train_model.py)
- format_chat: оставляет первый system, склеивает подряд идущие роли, убирает ведущие assistant, чередует user/assistant и обрезает хвост до user; prompt строится через tokenizer.apply_chat_template(..., add_generation_prompt=True) — корректно для шаблона Mistral, без ошибок чередования ролей.
- Токенизация и разбиение: ответ токенизируется полностью (усекается только если сам > лимита). Prompt режется на окна с перекрытием (stride = max_prompt_len//2) под общий лимит; хвост истории всегда включён. Каждый сэмпл имеет согласованные input_ids/attention_mask/labels; labels маскируют prompt (-100) и содержат только ответ.
- Результат после разбиения: train 3891 диалогов → 4047 сэмплов, valid 671 → 724 сэмпла, max длина 2048. Пропусков из-за длины нет; только экстремально длинные ответы могли быть усечены, но не отброшены.

Инференс ботом (3. Bot.py)
- История нормализуется тем же способом (build_chat_messages) и подаётся в tokenizer.apply_chat_template(..., add_generation_prompt=True), т.е. формат совпадает с обучением.
- Prompt токенизируется без спецтокенов; если длина > MAX_CONTEXT_TOKENS, обрезается справа (сохраняется свежий контекст). attention_mask собирается вручную, ответ выделяется после длины prompt.
- Таким образом, подача диалога на инференс соответствует формату обучения, контекст сохраняется максимально, ошибок шаблона нет.

Вывод
- Контекст диалогов учитывается правильно и соответствует требованиям шаблона Mistral.
- Разделение по токенам работает без потерь диалогов: длинные истории дробятся на окна, ответы сохраняются, лимит 2048 соблюдается.
